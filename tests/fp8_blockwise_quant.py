"""
Qwen3-Next-80B-A3B FP8 Blockwise æƒé‡é‡åŒ–è„šæœ¬

é‡åŒ–æ–¹æ³•ï¼š
    - æ ¼å¼ï¼šFP8 E4M3
    - å—å¤§å°ï¼š128x128
    - æ¿€æ´»æ–¹æ¡ˆï¼šåŠ¨æ€é‡åŒ–
    - ä¸ºæ¯ä¸ªæƒé‡ tensor ç”Ÿæˆå¯¹åº”çš„ weight_scale_inv å‚æ•°

é€‚ç”¨æ¨¡å‹ï¼š
    - Qwen3-Next-80B-A3B-Instruct (BF16)

ä½¿ç”¨ç¤ºä¾‹ï¼š
    åŸºç¡€ç”¨æ³•ï¼š
        python3 fp8_quant_qwen.py \\
            models/Qwen3-Next-80B-A3B-Instruct \\
            quantization_log.txt

    æŒ‡å®šè¾“å‡ºç›®å½•ï¼š
        python3 fp8_quant_qwen.py \\
            models/Qwen3-Next-80B-A3B-Instruct \\
            quantization_log.txt \\
            --output-dir models/My_FP8_Model

è¾“å‡ºå†…å®¹ï¼š
    - é‡åŒ–åçš„ safetensors æ–‡ä»¶ï¼ˆåŒ…å« weight å’Œ weight_scale_invï¼‰
    - å®Œæ•´çš„ config.jsonï¼ˆå« quantization_configï¼‰
    - tokenizer ç­‰å…¶ä»–å¿…è¦æ–‡ä»¶
    - é‡åŒ–æ—¥å¿—æ–‡ä»¶

é‡åŒ–é…ç½®å·²ç»è‡ªåŠ¨æ›´æ–°ï¼ˆå†™å…¥ config.jsonï¼‰ï¼š
    {
        "activation_scheme": "dynamic",
        "fmt": "e4m3",
        "quant_method": "fp8",
        "weight_block_size": [128, 128],
        "modules_to_not_convert": [...]
    }

ä½œè€…ï¼šxuhongbo02
ç‰ˆæœ¬ï¼š1.0
"""
from typing import Tuple
import torch
import triton
import triton.language as tl
import argparse
import os
import json
from safetensors.torch import load_file, save_file


@triton.jit
def fp8_blockwise_quant_act_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):
    """
    Quantizes the input tensor `x_ptr` and stores the result in `y_ptr` and the scaling factor in `s_ptr`.
    Args:
        x_ptr (triton.Pointer): Pointer to the input tensor.
        y_ptr (triton.Pointer): Pointer to the output tensor where quantized values will be stored.
        s_ptr (triton.Pointer): Pointer to the output tensor where scaling factors will be stored.
        BLOCK_SIZE (tl.constexpr): The size of the block to be processed by each program instance.
    Returns:
        None
    """
    pid = tl.program_id(axis=0)
    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    x = tl.load(x_ptr + offs).to(tl.float32)
    s = tl.max(tl.abs(x)) / 448.0
    y = x / s
    y = y.to(y_ptr.dtype.element_ty)
    tl.store(y_ptr + offs, y)
    tl.store(s_ptr + pid, s)


def fp8_blockwise_act_quant(x: torch.Tensor, block_size: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Quantizes the input tensor `x` using block-wise quantization.
    Args:
        x (torch.Tensor): The input tensor to be quantized. Must be contiguous and its last dimension size must be divisible by `block_size`.
        block_size (int, optional): The size of the blocks to be used for quantization. Default is 128.
    Returns:
        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:
            - The quantized tensor with dtype `torch.float8_e4m3fn`.
            - A tensor of scaling factors with dtype `torch.float32`.
    """
    assert x.is_contiguous(), 'Input tensor must be contiguous'
    assert x.size(-1) % block_size == 0, f'Last dimension size must be divisible by block_size (block_size={block_size})'
    y = torch.empty_like(x, dtype=torch.float8_e4m3fn)
    s = x.new_empty(*x.size()[:-1], x.size(-1) // block_size, dtype=torch.float32)
    grid = lambda meta: (triton.cdiv(x.numel(), meta['BLOCK_SIZE']), )
    fp8_blockwise_quant_act_kernel[grid](x, y, s, BLOCK_SIZE=block_size)
    return y, s

@triton.jit
def fp8_blockwise_quant_weight_kernel(x_ptr, y_ptr, s_ptr, M, N, BLOCK_SIZE: tl.constexpr):
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)
    n = tl.cdiv(N, BLOCK_SIZE)
    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    offs_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    offs = offs_m[:, None] * N + offs_n[None, :]
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    x = tl.load(x_ptr + offs, mask=mask).to(tl.float32)
    s = tl.max(tl.abs(x)) / 448.0
    y = x / s
    y = y.to(y_ptr.dtype.element_ty)
    tl.store(y_ptr + offs, y, mask=mask)
    tl.store(s_ptr + pid_m * n + pid_n, s)


def fp8_blockwise_weight_quant(x: torch.Tensor, block_size: int = 128):
    assert x.is_contiguous(), 'Input tensor must be contiguous'
    assert x.dim() == 2, 'Input tensor must have 2 dimensions'
    M, N = x.size()
    y = torch.empty_like(x, dtype=torch.float8_e4m3fn)
    s_rows = triton.cdiv(M, block_size)
    s_cols = triton.cdiv(N, block_size)
    s = x.new_empty(s_rows, s_cols, dtype=torch.float32)
    grid = lambda meta: (s_rows, s_cols)
    fp8_blockwise_quant_weight_kernel[grid](x, y, s, M, N, BLOCK_SIZE=block_size)
    return y, s

@triton.jit
def fp8_blockwise_dequant_weight_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):
    """
    Dequantizes weights using the provided scaling factors and stores the result.
    Args:
        x_ptr (tl.pointer): Pointer to the quantized weights.
        s_ptr (tl.pointer): Pointer to the scaling factors.
        y_ptr (tl.pointer): Pointer to the output buffer for dequantized weights.
        M (int): Number of rows in the weight matrix.
        N (int): Number of columns in the weight matrix.
        BLOCK_SIZE (tl.constexpr): Size of the block for tiling.
    Returns:
        None
    """
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)
    n = tl.cdiv(N, BLOCK_SIZE)
    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    offs_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    offs = offs_m[:, None] * N + offs_n[None, :]
    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)
    x = tl.load(x_ptr + offs, mask=mask).to(tl.float32)
    s = tl.load(s_ptr + pid_m * n + pid_n)
    y = x * s
    tl.store(y_ptr + offs, y, mask=mask)


def fp8_blockwise_weight_dequant(x: torch.Tensor, s: torch.Tensor, block_size: int = 128) -> torch.Tensor:
    """
    Dequantizes the given weight tensor using the provided scale tensor.
    Args:
        x (torch.Tensor): The quantized weight tensor of shape (M, N).
        s (torch.Tensor): The scale tensor of shape (M, N).
        block_size (int, optional): The block size to use for dequantization. Defaults to 128.
    Returns:
        torch.Tensor: The dequantized weight tensor of the same shape as `x`.
    Raises:
        AssertionError: If `x` or `s` are not contiguous or if their dimensions are not 2.
    """
    assert x.is_contiguous() and s.is_contiguous(), 'Input tensors must be contiguous'
    assert x.dim() == 2 and s.dim() == 2, 'Input tensors must have 2 dimensions'
    M, N = x.size()
    y = torch.empty_like(x, dtype=torch.get_default_dtype())
    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE']), triton.cdiv(N, meta['BLOCK_SIZE']))
    fp8_blockwise_dequant_weight_kernel[grid](x, s, y, M, N, BLOCK_SIZE=block_size)
    return y


# å…¨å±€é‡åŒ–é…ç½® - åŸºäº Qwen3-Next-80B-A3B-Instruct-FP8 å®˜æ–¹æ¨¡å‹
DEFAULT_QUANT_CONFIG = {
    "activation_scheme": "dynamic",
    "fmt": "e4m3",
    "quant_method": "fp8",
    "weight_block_size": [128, 128],
    "modules_to_not_convert": [
        "lm_head",
        "model.embed_tokens",
        # Layer 0-47 çš„æ’é™¤æ¨¡å—ï¼ˆ48å±‚ï¼‰
        *[f"model.layers.{i}.input_layernorm" for i in range(48)],
        *[f"model.layers.{i}.post_attention_layernorm" for i in range(48)],
        *[f"model.layers.{i}.mlp.gate" for i in range(48)],
        *[f"model.layers.{i}.mlp.shared_expert_gate" for i in range(48)],
        # Linear attention å±‚ï¼ˆå±‚ 0,1,2,4,5,6,8,9,10,12,13,14,16,17,18,20,21,22,24,25,26,28,29,30,32,33,34,36,37,38,40,41,42,44,45,46ï¼‰
        *[f"model.layers.{i}.linear_attn.A_log" for i in range(48) if i % 4 != 3],
        *[f"model.layers.{i}.linear_attn.conv1d" for i in range(48) if i % 4 != 3],
        *[f"model.layers.{i}.linear_attn.dt_bias" for i in range(48) if i % 4 != 3],
        *[f"model.layers.{i}.linear_attn.in_proj_ba" for i in range(48) if i % 4 != 3],
        *[f"model.layers.{i}.linear_attn.norm" for i in range(48) if i % 4 != 3],
        # Self attention å±‚ï¼ˆå±‚ 3,7,11,15,19,23,27,31,35,39,43,47ï¼‰
        *[f"model.layers.{i}.self_attn.k_norm" for i in range(3, 48, 4)],
        *[f"model.layers.{i}.self_attn.q_norm" for i in range(3, 48, 4)],
        # MTP (Multi-Token Prediction) æ¨¡å—
        "mtp.fc",
        "mtp.norm",
        "mtp.pre_fc_norm_embedding",
        "mtp.pre_fc_norm_hidden",
        "mtp.layers.0.input_layernorm",
        "mtp.layers.0.mlp.gate",
        "mtp.layers.0.mlp.shared_expert_gate",
        "mtp.layers.0.post_attention_layernorm",
        "mtp.layers.0.self_attn.k_norm",
        "mtp.layers.0.self_attn.q_norm",
    ]
}

def process_models(model_dir, output_dir, info_path):
    """
    å¤„ç†æ¨¡å‹ç›®å½•ä¸­çš„æ‰€æœ‰safetensorsæ–‡ä»¶ï¼Œå¯¹ç¬¦åˆæ¡ä»¶çš„æƒé‡è¿›è¡ŒFP8é‡åŒ–
    
    Args:
        model_dir: åŒ…å«safetensorsæ–‡ä»¶çš„ç›®å½•è·¯å¾„
        output_dir: é‡åŒ–åæ¨¡å‹ä¿å­˜ç›®å½•
        info_path: ä¿®æ”¹è®°å½•ä¿å­˜è·¯å¾„
    """
    os.makedirs(output_dir, exist_ok=True)
    modified_log = []
    
    # ä½¿ç”¨å…¨å±€é…ç½®
    print("â„¹ï¸  ä½¿ç”¨å†…ç½®çš„é»˜è®¤é‡åŒ–é…ç½®")
    excluded_modules = set(DEFAULT_QUANT_CONFIG["modules_to_not_convert"])
    print(f"âœ“ åŠ è½½äº† {len(excluded_modules)} ä¸ªæ’é™¤æ¨¡å—")

    for filename in os.listdir(model_dir):
        if not filename.endswith(".safetensors"):
            continue

        filepath = os.path.join(model_dir, filename)
        output_filepath = os.path.join(output_dir, filename)
        print(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")

        try:
            tensors = load_file(filepath)
            modified = False
            # print(list(tensors.keys()))  # è°ƒè¯•ç”¨ï¼Œå·²æ³¨é‡Š
            for key in list(tensors.keys()):
                tensor = tensors[key].to("cuda")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡åŒ–ï¼ˆä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼‰
                should_quantize = False
                if 'weight' in key:
                    param_name = key.replace('.weight', '')
                    should_quantize = param_name not in excluded_modules
                
                if should_quantize:
                    # è·³è¿‡é2Då¼ é‡
                    if tensor.dim() != 2:
                        print(f"   âš ï¸ è·³è¿‡é2Då¼ é‡: {key}")
                        continue
                    
                    # æ‰§è¡Œé‡åŒ–
                    try:
                        quantized, scale = fp8_blockwise_weight_quant(tensor)
                        
                        # æ›´æ–°å¼ é‡å­—å…¸
                        tensors[key] = quantized.cpu()
                        scale_key = f"{key}_scale_inv"
                        tensors[scale_key] = scale.cpu()
                        
                        # è®°å½•ä¿®æ”¹
                        log_entry = f"{filename} | {key} | {tensor.shape}â†’é‡åŒ– | scale_shape: {scale.shape}"
                        modified_log.append(log_entry)
                        modified = True
                        print(f"   â†’ é‡åŒ–æˆåŠŸ: {key} | scaleå½¢çŠ¶: {scale.shape}")
                    
                    except Exception as e:
                        print(f"   âŒ é‡åŒ–å¤±è´¥ {key}: {str(e)}")
                        continue

            save_file(tensors, output_filepath)
            print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_filepath}\n")

        except Exception as e:
            print(f"âŒ å¤„ç† {filename} æ—¶å‡ºé”™: {str(e)}\n")
            continue
    
    # ä¿å­˜ä¿®æ”¹è®°å½•
    with open(info_path, "w") as f:
        f.write("æ–‡ä»¶ | Tensoråç§° | ä¿®æ”¹è®°å½•\n")
        f.write("\n".join(modified_log))

    print(f"âœ… å¤„ç†å®Œæˆï¼å…±ä¿®æ”¹ {len(modified_log)} ä¸ªtensor")
    print(f"ğŸ“ ä¿®æ”¹è®°å½•å·²ä¿å­˜è‡³: {info_path}")
    
    # å¤åˆ¶é safetensors æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    print("\nğŸ”§ å¤åˆ¶å…¶ä»–å¿…è¦æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•...")
    copied_files = []
    for filename in os.listdir(model_dir):
        if not filename.endswith(".safetensors"):
            src = os.path.join(model_dir, filename)
            dst = os.path.join(output_dir, filename)
            try:
                if os.path.isdir(src):
                    continue  # è·³è¿‡ç›®å½•
                import shutil
                shutil.copy2(src, dst)
                copied_files.append(filename)
            except Exception as e:
                print(f"    âŒ å¤åˆ¶ {filename} å¤±è´¥: {e}")
    
    print(f"âœ“ å·²å¤åˆ¶ {len(copied_files)} ä¸ªæ–‡ä»¶")
    
    # å°†é‡åŒ–é…ç½®å†™å…¥è¾“å‡ºç›®å½•çš„config.json
    print("\nğŸ“ å†™å…¥é‡åŒ–é…ç½®...")
    output_config_path = os.path.join(output_dir, "config.json")
    try:
        with open(output_config_path, 'r') as f:
            model_config = json.load(f)
        model_config["quantization_config"] = DEFAULT_QUANT_CONFIG
        with open(output_config_path, 'w') as f:
            json.dump(model_config, f, indent=2)
        print(f"âœ“ å·²å°†é‡åŒ–é…ç½®å†™å…¥ {output_config_path}")
    except Exception as e:
        print(f"âš ï¸ å†™å…¥é‡åŒ–é…ç½®å¤±è´¥: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æƒé‡FP8é‡åŒ–å·¥å…·')
    parser.add_argument('model_dir', help='æ¨¡å‹æ–‡ä»¶æ‰€åœ¨ç›®å½•')
    parser.add_argument('info_path', help='é‡åŒ–è®°å½•ä¿å­˜è·¯å¾„')
    parser.add_argument('--output-dir', help='é‡åŒ–åæ¨¡å‹ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºåŸç›®å½•åŠ _fp8_quant', default=None)
    args = parser.parse_args()
    
    # å¤„ç†è¾“å‡ºç›®å½•é€»è¾‘
    if args.output_dir is None:
        args.output_dir = f"{args.model_dir.rstrip('/')}_fp8_quant"
    
    process_models(args.model_dir, args.output_dir, args.info_path)
